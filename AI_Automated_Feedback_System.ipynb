{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV5pmuX7k9rqkjFZ7ql7AQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrPrabhathPeri/AI-Feedback-System/blob/main/AI_Automated_Feedback_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uzdPS0n0PyeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6aa916-239a-4faf-d09d-71720a429984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Loading 3 separate API keys...\n",
            "‚úÖ Successfully loaded all 3 API keys.\n",
            "üöÄ Using Groq Model: llama-3.3-70b-versatile\n",
            "‚úÖ Loaded dataset. Running on 200 rows.\n",
            "\n",
            "üöÄ Running Zero Shot...\n",
            "\n",
            "‚úÖ Zero Shot Done. Accuracy: 63.50% | Validity: 100.00%\n",
            "\n",
            "üöÄ Running Few Shot...\n",
            "\n",
            "‚úÖ Few Shot Done. Accuracy: 60.50% | Validity: 100.00%\n",
            "\n",
            "üöÄ Running Chain of Thought...\n",
            "\n",
            "‚úÖ Chain of Thought Done. Accuracy: 59.50% | Validity: 100.00%\n",
            "\n",
            "üìä FINAL SCORES\n",
            "           Approach  Accuracy  JSON Validity\n",
            "0         Zero Shot     0.635            1.0\n",
            "1          Few Shot     0.605            1.0\n",
            "2  Chain of Thought     0.595            1.0\n",
            "\n",
            "‚úÖ DONE! Download the CSV files.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. INSTALL GROQ CLIENT ---\n",
        "!pip install -q groq\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "from groq import Groq\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 2. SETUP (3 SEPARATE KEYS) ---\n",
        "print(\"üîë Loading 3 separate API keys...\")\n",
        "\n",
        "try:\n",
        "    # Key for Zero Shot\n",
        "    KEY_1 = userdata.get('GROQ_API_KEY_1')\n",
        "    client_1 = Groq(api_key=KEY_1)\n",
        "\n",
        "    # Key for Few Shot\n",
        "    KEY_2 = userdata.get('GROQ_API_KEY_2')\n",
        "    client_2 = Groq(api_key=KEY_2)\n",
        "\n",
        "    # Key for Chain of Thought\n",
        "    KEY_3 = userdata.get('GROQ_API_KEY_3')\n",
        "    client_3 = Groq(api_key=KEY_3)\n",
        "\n",
        "    print(\"‚úÖ Successfully loaded all 3 API keys.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading keys. Make sure GROQ_API_KEY_1, _2, and _3 are in Secrets.\")\n",
        "    # Stop execution if keys are missing to avoid errors later\n",
        "    raise e\n",
        "\n",
        "# MODEL NAME\n",
        "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
        "print(f\"üöÄ Using Groq Model: {MODEL_NAME}\")\n",
        "\n",
        "# --- 3. DATA LOAD ---\n",
        "try:\n",
        "    full_df = pd.read_csv('yelp.csv')\n",
        "    full_df['stars'] = pd.to_numeric(full_df['stars'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Run on 200 rows\n",
        "    df = full_df.sample(n=200, random_state=42).reset_index(drop=True)\n",
        "    print(f\"‚úÖ Loaded dataset. Running on {len(df)} rows.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading file: {e}\")\n",
        "    df = pd.DataFrame([{\"text\": \"Good\", \"stars\": 5}, {\"text\": \"Bad\", \"stars\": 1}])\n",
        "\n",
        "# --- 4. HELPER FUNCTION (Accepts specific client) ---\n",
        "def get_groq_prediction(client, prompt):\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODEL_NAME,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that outputs ONLY JSON.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è API Error: {e}\")\n",
        "        time.sleep(2)\n",
        "        return None\n",
        "\n",
        "# --- 5. PROMPTS ---\n",
        "\n",
        "def get_prompt_zero_shot(review_text):\n",
        "    return f\"\"\"\n",
        "    You are an expert Review Sentiment Classifier. Your task is to analyze the user's review and assign a precise star rating (1-5).\n",
        "\n",
        "    Rating Guide:\n",
        "    - 1 Star: Completely negative experience, angry customer, severe issues.\n",
        "    - 2 Stars: Mostly negative, but one redeemable quality.\n",
        "    - 3 Stars: Mixed feelings. Good food but bad service, or average experience.\n",
        "    - 4 Stars: Very good, but not perfect.\n",
        "    - 5 Stars: Outstanding, highly recommended, no complaints.\n",
        "\n",
        "    Output STRICT JSON format: {{\"predicted_stars\": int, \"explanation\": \"string\"}}\n",
        "\n",
        "    Review: \"{review_text}\"\n",
        "    \"\"\"\n",
        "\n",
        "def get_prompt_few_shot(review_text):\n",
        "    return f\"\"\"\n",
        "    You are a Yelp Rating AI. Classify the review into 1-5 stars based on these examples.\n",
        "\n",
        "    Examples:\n",
        "    Input: \"The waiter was rude and the food was cold. I'm never coming back!\"\n",
        "    Output: {{\"predicted_stars\": 1, \"explanation\": \"Customer expresses anger about both service and food quality. Strong negative sentiment.\"}}\n",
        "\n",
        "    Input: \"The burger was tasty, but we had to wait 45 minutes for a table. It was just okay.\"\n",
        "    Output: {{\"predicted_stars\": 3, \"explanation\": \"Mixed sentiment. Positive food comment is outweighed by negative service experience.\"}}\n",
        "\n",
        "    Input: \"Absolutely incredible! The steak was cooked perfectly and the ambiance was lovely.\"\n",
        "    Output: {{\"predicted_stars\": 5, \"explanation\": \"Glowing review with no negatives mentioned. High enthusiasm.\"}}\n",
        "\n",
        "    Task:\n",
        "    Input: \"{review_text}\"\n",
        "    Output:\n",
        "    \"\"\"\n",
        "\n",
        "def get_prompt_cot(review_text):\n",
        "    return f\"\"\"\n",
        "    Analyze the following review using a Step-by-Step Chain of Thought process.\n",
        "\n",
        "    Steps:\n",
        "    1. Identify positive mentions (e.g., food taste, ambiance).\n",
        "    2. Identify negative mentions (e.g., wait time, rude staff).\n",
        "    3. Weigh the positives against the negatives to determine the final score.\n",
        "\n",
        "    Return the final rating in JSON format: {{\"predicted_stars\": int, \"explanation\": \"Detailed reasoning based on the steps above.\"}}\n",
        "\n",
        "    Review: \"{review_text}\"\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "# --- 6. EXPERIMENT ENGINE (Accepts specific client) ---\n",
        "def run_experiment(name, prompt_func, dataset, specific_client):\n",
        "    results = []\n",
        "    print(f\"\\nüöÄ Running {name}...\")\n",
        "\n",
        "    for i, row in dataset.iterrows():\n",
        "        # Pass the specific client (key) for this experiment\n",
        "        json_response = get_groq_prediction(specific_client, prompt_func(row['text']))\n",
        "\n",
        "        valid = False\n",
        "        pred = 0\n",
        "        explanation = \"N/A\"\n",
        "\n",
        "        if json_response:\n",
        "            try:\n",
        "                parsed = json.loads(json_response)\n",
        "                pred = int(parsed['predicted_stars'])\n",
        "                explanation = parsed.get('explanation', \"No explanation found\")\n",
        "                valid = True\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        results.append({\n",
        "            \"review_text\": row['text'],\n",
        "            \"actual\": int(row['stars']),\n",
        "            \"predicted\": pred,\n",
        "            \"explanation\": explanation,\n",
        "            \"valid_json\": valid\n",
        "        })\n",
        "\n",
        "        if i % 10 == 0: print(f\"   Processed {i+1}/{len(dataset)}...\", end=\"\\r\")\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Calculate Accuracy\n",
        "    valid_df = results_df[results_df['valid_json'] == True]\n",
        "    if not valid_df.empty:\n",
        "        acc = accuracy_score(valid_df['actual'], valid_df['predicted'])\n",
        "    else:\n",
        "        acc = 0.0\n",
        "\n",
        "    validity = results_df['valid_json'].mean()\n",
        "\n",
        "    print(f\"\\n‚úÖ {name} Done. Accuracy: {acc:.2%} | Validity: {validity:.2%}\")\n",
        "    return results_df, acc, validity\n",
        "\n",
        "# --- 7. EXECUTE (Using 3 different keys) ---\n",
        "\n",
        "# Experiment 1: Zero Shot using Key 1\n",
        "res_zero, acc_zero, val_zero = run_experiment(\"Zero Shot\", get_prompt_zero_shot, df, client_1)\n",
        "\n",
        "# Experiment 2: Few Shot using Key 2\n",
        "res_few, acc_few, val_few = run_experiment(\"Few Shot\", get_prompt_few_shot, df, client_2)\n",
        "\n",
        "# Experiment 3: CoT using Key 3\n",
        "res_cot, acc_cot, val_cot = run_experiment(\"Chain of Thought\", get_prompt_cot, df, client_3)\n",
        "\n",
        "# Save Files\n",
        "res_zero.to_csv(\"results_zero_shot.csv\", index=False)\n",
        "res_few.to_csv(\"results_few_shot.csv\", index=False)\n",
        "res_cot.to_csv(\"results_cot.csv\", index=False)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"Approach\": [\"Zero Shot\", \"Few Shot\", \"Chain of Thought\"],\n",
        "    \"Accuracy\": [acc_zero, acc_few, acc_cot],\n",
        "    \"JSON Validity\": [val_zero, val_few, val_cot]\n",
        "})\n",
        "comparison_df.to_csv(\"final_comparison.csv\", index=False)\n",
        "\n",
        "print(\"\\nüìä FINAL SCORES\")\n",
        "print(comparison_df)\n",
        "print(\"\\n‚úÖ DONE! Download the CSV files.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1tSdT_hREBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}